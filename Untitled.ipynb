{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431ee46a-f438-4276-ad3e-0ece7b59e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Config:\n",
      "-----------------------------------------\n",
      "AUG:\n",
      "  RC_PROB: 0.5\n",
      "  RE_PROB: 0.5\n",
      "  RF_PROB: 0.5\n",
      "DATA:\n",
      "  DATASET: prcc\n",
      "  HEIGHT: 384\n",
      "  NUM_INSTANCES: 8\n",
      "  NUM_WORKERS: 4\n",
      "  ROOT: /root/autodl-tmp/datasets\n",
      "  TEST_BATCH: 128\n",
      "  TRAIN_BATCH: 32\n",
      "  WIDTH: 192\n",
      "EVAL_MODE: False\n",
      "GPU: 0,1\n",
      "INFER:\n",
      "  SHOW_CC: True\n",
      "INFER_MODE: False\n",
      "LOSS:\n",
      "  CAL: cal\n",
      "  CLA_LOSS: crossentropylabelsmooth\n",
      "  CLA_M: 0.0\n",
      "  CLA_S: 16.0\n",
      "  CLOTHES_CLA_LOSS: cosface\n",
      "  EPSILON: 0.1\n",
      "  MOMENTUM: 0.0\n",
      "  PAIR_LOSS: triplet\n",
      "  PAIR_LOSS_WEIGHT: 0.0\n",
      "  PAIR_M: 0.3\n",
      "  PAIR_S: 16.0\n",
      "MODEL:\n",
      "  CLOTHES_DIM: 1024\n",
      "  CONTOUR_DIM: 1024\n",
      "  FEATURE_DIM: 4096\n",
      "  HID_REC_DIM: 4096\n",
      "  NAME: resnet50\n",
      "  NO_CLOTHES_DIM: 2048\n",
      "  POOLING:\n",
      "    NAME: maxavg\n",
      "    P: 3\n",
      "  RES4_STRIDE: 1\n",
      "  RESUME: /root/DCR-ReID/logs/prcc/res50-cels-cal/best_model.pth.tar\n",
      "OUTPUT: ./logs/prcc/res50-cels-cal/1.0-0.05-1.0\n",
      "PARA:\n",
      "  RECON_RATIO: 1.0\n",
      "  SHUF_ADV_CLO_RATIO: 0.05\n",
      "  SHUF_PID_RATIO: 1.0\n",
      "SEED: 1\n",
      "TAG: res50-cels-cal\n",
      "TEST:\n",
      "  EVAL_STEP: 5\n",
      "  START_EVAL: 0\n",
      "TRAIN:\n",
      "  AMP: False\n",
      "  LR_SCHEDULER:\n",
      "    DECAY_RATE: 0.1\n",
      "    STEPSIZE: [20, 40, 60]\n",
      "  MAX_EPOCH: 80\n",
      "  OPTIMIZER:\n",
      "    LR: 0.00035\n",
      "    NAME: adam\n",
      "    WEIGHT_DECAY: 0.0005\n",
      "  START_EPOCH: 0\n",
      "  START_EPOCH_ADV: 25\n",
      "  START_EPOCH_CC: 25\n",
      "-----------------------------------------\n",
      "=> PRCC loaded\n",
      "Dataset statistics:\n",
      "  --------------------------------------------\n",
      "  subset      | # ids | # images | # clothes\n",
      "  --------------------------------------------\n",
      "  train       |   150 |    17896 |       300\n",
      "  val         |   150 |     5002 |       300\n",
      "  test        |    71 |    10800 |       142\n",
      "  query(same) |    71 |     3873 |\n",
      "  query(diff) |    71 |     3543 |\n",
      "  gallery     |    71 |     3384 |\n",
      "  --------------------------------------------\n",
      "  total       |   221 |    33698 |       442\n",
      "  --------------------------------------------\n",
      "Initializing model: resnet50\n",
      "Init model: 'resnet50'\n",
      "Model size: 25.53175M\n",
      "Loading checkpoint from '/root/DCR-ReID/logs/prcc/res50-cels-cal/best_model.pth.tar'\n",
      "==> Start training\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape:f_unclo.shape:  torch.Size([32, 1024, 24, 12])torch.Size([32, 1024, 24, 12])\n",
      "\n",
      "f_cont.shape:f_cont.shape:  torch.Size([32, 1024, 24, 12])torch.Size([32, 1024, 24, 12])\n",
      "\n",
      "f_clo.shape:f_clo.shape:  torch.Size([32, 512, 24, 12])torch.Size([32, 512, 24, 12])\n",
      "\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: f_unclo.shape:torch.Size([32, 1024, 24, 12]) \n",
      "f_cont.shape:torch.Size([32, 1024, 24, 12]) \n",
      "torch.Size([32, 1024, 24, 12])f_cont.shape:\n",
      " f_clo.shape: torch.Size([32, 1024, 24, 12])\n",
      "torch.Size([32, 512, 24, 12])f_clo.shape:\n",
      " torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "f_unclo.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_cont.shape: torch.Size([32, 1024, 24, 12])\n",
      "f_clo.shape: torch.Size([32, 512, 24, 12])\n",
      "^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 966 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 967 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 223, in <module>\n",
      "    main(config)\n",
      "  File \"main.py\", line 159, in main\n",
      "    train_cal(config, epoch, model, classifier, clothes_classifier, criterion_cla, criterion_pair, \n",
      "  File \"/root/DCR-ReID/train.py\", line 97, in train_cal\n",
      "    loss.backward()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 363, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 223, in <module>\n",
      "    main(config)\n",
      "  File \"main.py\", line 159, in main\n",
      "    train_cal(config, epoch, model, classifier, clothes_classifier, criterion_cla, criterion_pair, \n",
      "  File \"/root/DCR-ReID/train.py\", line 97, in train_cal\n",
      "    loss.backward()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 363, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=2 --master_port 12345 main.py --dataset prcc --cfg configs/res50_cels_cal.yaml --gpu 0,1 --spr 1.0 --sacr 0.05 --rr 1.0 --resume '/root/DCR-ReID/logs/prcc/res50-cels-cal/best_model.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1118e-af48-42b8-85f5-f70a3ab553e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
